{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LDBBQxwMKz1"
      },
      "source": [
        "# FTIR-Evs Data anlysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtGvh-yd1A0j"
      },
      "source": [
        "1 - Loading the data and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQKMIhEqM2em"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "\n",
        "ftir_data =pd.read_csv('3500-1000_emsc-mean_cut_3500-2800_1800-1000.csv')\n",
        "# drop the first 5 cols \n",
        "ftir_data1 = ftir_data.drop(ftir_data.columns[0:5], axis=1)\n",
        "ftir_data2 = ftir_data1.drop(ftir_data1.columns[1:31], axis=1)\n",
        "ftir_data3 = ftir_data2.drop(ftir_data2.columns[2:5], axis=1)\n",
        "ftir_data4 = ftir_data3.drop(ftir_data3.columns[1], axis=1)\n",
        "\n",
        "\n",
        "# get mean for each pid replicate \n",
        "ftir_data5 = ftir_data4.groupby('pid').mean()\n",
        "ftir_data6 = ftir_data3.drop(ftir_data4.columns[1:1504], axis=1)\n",
        "\n",
        "# merge ftir_data5 with ftir_data6 get class \n",
        "ftir_data7 = pd.merge(ftir_data5, ftir_data6, on='pid', how='inner')\n",
        "# drop dublicate rows\n",
        "ftir_data8 = ftir_data7.drop_duplicates()\n",
        "\n",
        "\n",
        "# from pid string replace EV with '' \n",
        "ftir_data8['pid'] = ftir_data8['pid'].str.replace('EV', '')\n",
        "\n",
        "# # save to csv\n",
        "ftir_data8.to_csv('processed_ftir_data.csv', index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ftir_data8['target'] = ftir_data8['pid'].apply(lambda x: 0 if 'H' in x else 1) # 0 for Healthy and 1 for GBM\n",
        "ftir_data8 = ftir_data8.drop(['Class'], axis = 1)\n",
        "\n",
        "# count the number of healthy and GBM\n",
        "print(ftir_data8['target'].value_counts())\n",
        "\n",
        "# change H38rpt to H38 in pid column\n",
        "FTIR_data = ftir_data8.copy()\n",
        "FTIR_data['pid'] = FTIR_data['pid'].str.replace('H38', 'H38rpt', regex=False)\n",
        "# Ensure pid columns have the same data type\n",
        "FTIR_data['pid'] = FTIR_data['pid'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FTIR_data.set_index('pid', inplace=True)\n",
        "FTIR_data = FTIR_data.reset_index(drop=True)\n",
        "# \n",
        "X = FTIR_data.drop(['target',], axis = 1)\n",
        "y = FTIR_data['target']\n",
        "\n",
        "# scale X \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1a. Validation Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read validation data\n",
        "validation_data = pd.read_csv('FTIR preprocessed-longitudinal.csv')\n",
        "# use column 'pid' and keep pid that are here 'B1', 'C1', 'D1', 'E1', 'F1'\n",
        "validation_data = validation_data[validation_data['pid'].isin(['B1', 'C1', 'D1', 'E1', 'F1'])]\n",
        "validation_data.set_index('pid', inplace=True)\n",
        "# drop first 40 columns \n",
        "validation_data = validation_data.drop(validation_data.columns[0:39], axis=1)\n",
        "# drop last 23 columns\n",
        "validation_data = validation_data.drop(validation_data.columns[-23:], axis=1)\n",
        "# get mean for each pid replicate \n",
        "validation_data = validation_data.groupby('pid').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForest Feature Selection Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "cv_split = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_rfecv = RFECV(\n",
        "    estimator=rf,\n",
        "    step=1,\n",
        "    min_features_to_select=1,\n",
        "    cv=cv_split,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "rf_rfecv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal number of features: {rf_rfecv.n_features_}\")\n",
        "print(f\"Optimal features: {list(rf_rfecv.get_feature_names_out())}\")\n",
        "\n",
        "rf_rfecv.cv_results_\n",
        "\n",
        "def plot_features_vs_cvscore_95ci(rfecv_model, cv_folds=5):\n",
        "    n_features = len(rfecv_model.cv_results_[\"mean_test_score\"])\n",
        "    mean_scores = rfecv_model.cv_results_[\"mean_test_score\"]\n",
        "    std_scores = rfecv_model.cv_results_[\"std_test_score\"]\n",
        "\n",
        "    # Calculate 95% CI\n",
        "    ci = 1.96 * (std_scores / np.sqrt(cv_folds))\n",
        "\n",
        "    x_range = range(1, n_features + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(x_range, mean_scores, marker='o', label='Mean ROC AUC')\n",
        "    plt.fill_between(x_range,\n",
        "                     mean_scores - ci,\n",
        "                     mean_scores + ci,\n",
        "                     color='lightblue',\n",
        "                     alpha=0.4,\n",
        "                     label='95% Confidence Interval')\n",
        "\n",
        "    # Annotate best feature point\n",
        "    best_idx = np.argmax(mean_scores)\n",
        "    best_score = mean_scores[best_idx]\n",
        "    best_n_features = best_idx + 1\n",
        "    plt.axvline(best_n_features, color='red', linestyle='--', label=f'Best = {best_n_features} features')\n",
        "    plt.scatter(best_n_features, best_score, color='red')\n",
        "    plt.text(best_n_features + 10, best_score,\n",
        "             f'Best = {best_n_features}\\nROC AUC = {best_score:.3f}',\n",
        "             color='black', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    # Annotate point at 10 features \n",
        "    if n_features >= 10:\n",
        "        auc_at_10 = mean_scores[9]\n",
        "        plt.scatter(10, auc_at_10, color='green')\n",
        "        plt.text(10 + 10, auc_at_10,\n",
        "                 f'10 features\\nROC AUC = {auc_at_10:.3f}',\n",
        "                 color='darkgreen', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    plt.xlabel(\"Number of Features Selected\", fontsize=12)\n",
        "    plt.ylabel(\"Mean CV ROC_AUC\", fontsize=12)\n",
        "    plt.title(\"ROC_AUC vs Number of Features (with 95% CI)\", fontsize=14)\n",
        "\n",
        "    plt.xticks(np.arange(0, n_features + 1, step=100))\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_features_vs_cvscore_95ci(rf_rfecv, cv_folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2a. RandomForest ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize result lists\n",
        "AUC, AUC_sd, AC, AC_sd, Pre, Pre_sd, Re, Re_sd, f1, f1_sd, Best1, Best2 = ([] for _ in range(12))\n",
        "\n",
        "# Reset indices of DataFrame\n",
        "X_scaled, y = X_scaled.reset_index(drop=True), y.reset_index(drop=True)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Use only the selected features\n",
        "rf_train_selected, rf_test_selected = X_train[list(rf_rfecv.get_feature_names_out())[:10]], X_test[list(rf_rfecv.get_feature_names_out())[:10]]\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [1, 2, 3, 5, None]}\n",
        "gs = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                  param_grid=param_grid,\n",
        "                  scoring='accuracy', cv=5, refit=True, n_jobs=-1)\n",
        "gs.fit(rf_train_selected, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = gs.best_params_\n",
        "print(f'Best Parameters: {best_params}')\n",
        "\n",
        "# Train final model\n",
        "rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "rf_model.fit(rf_train_selected, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "test_pred = rf_model.predict(rf_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels=np.unique(y))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=[str(label) for label in np.unique(y)]))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(rf_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "sd_list = []\n",
        "\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = rf_model.fit(rf_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(rf_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_auc, mean_sd = auc(mean_fpr, mean_tpr), np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, rf_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, rf_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, rf_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, rf_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(best_params['n_estimators'])\n",
        "Best2.append(best_params['max_depth'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3a. Knn model training using RFE (top-10 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recressive Feature Elimination\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize result lists\n",
        "AUC = []\n",
        "AUC_sd = []\n",
        "AC = []\n",
        "AC_sd = []\n",
        "Pre = []\n",
        "Pre_sd = []\n",
        "Re = []\n",
        "Re_sd = []\n",
        "f1 = []\n",
        "f1_sd = []\n",
        "Best1 = []\n",
        "Best2 = []\n",
        "\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top 10 features\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "selector = RFE(rf, n_features_to_select=10)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "top_10_features = X_train.columns[selector.support_]\n",
        "print(\"Top 10 Selected Features:\", top_10_features)\n",
        "\n",
        "# Use only the selected features for training\n",
        "Knn_train_selected = X_train[list(top_10_features)]\n",
        "Knn_test_selected = X_test[list(top_10_features)]\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [3, 5, 7, 9]\n",
        "gs = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                  param_grid=[{'n_neighbors': param_range1, 'metric': ['minkowski']}],\n",
        "                  scoring='accuracy', cv=2, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(Knn_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_n_neighbors = gs.best_params_['n_neighbors']\n",
        "print(f'Best n_neighbors: {best_n_neighbors}')\n",
        "\n",
        "# Evaluate model using the best parameters\n",
        "knn_model = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn_model.fit(Knn_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = knn_model.predict(Knn_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels=np.unique(y))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=[str(label) for label in np.unique(y)]))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(Knn_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "all_tpr = []\n",
        "\n",
        "sd_list = []\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = knn_model.fit(Knn_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(Knn_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "mean_sd = np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "# Plot mean ROC\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, Knn_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, Knn_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, Knn_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, Knn_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(gs.best_params_['n_neighbors'])\n",
        "Best2.append('NaN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGB Feature selection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize model\n",
        "\n",
        "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "cv_split = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "\n",
        "xgb_rfecv = RFECV(\n",
        "    estimator=xgboost_model,\n",
        "    step=1,\n",
        "    min_features_to_select=1,\n",
        "    cv=cv_split,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_rfecv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal number of features: {xgb_rfecv.n_features_}\")\n",
        "print(f\"Optimal features: {list(xgb_rfecv.get_feature_names_out())}\")\n",
        "\n",
        "xgb_rfecv.cv_results_\n",
        "\n",
        "def plot_features_vs_cvscore_95ci(rfecv_model, cv_folds=5):\n",
        "    n_features = len(rfecv_model.cv_results_[\"mean_test_score\"])\n",
        "    mean_scores = rfecv_model.cv_results_[\"mean_test_score\"]\n",
        "    std_scores = rfecv_model.cv_results_[\"std_test_score\"]\n",
        "\n",
        "    # Calculate 95% CI\n",
        "    ci = 1.96 * (std_scores / np.sqrt(cv_folds))\n",
        "\n",
        "    x_range = range(1, n_features + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(x_range, mean_scores, marker='o', label='Mean ROC AUC')\n",
        "    plt.fill_between(x_range,\n",
        "                 np.clip(mean_scores - ci, 0, 1),\n",
        "                 np.clip(mean_scores + ci, 0, 1),\n",
        "                 color='lightblue',\n",
        "                 alpha=0.4,\n",
        "                 label='95% Confidence Interval')\n",
        "\n",
        "\n",
        "    # Annotate best feature point\n",
        "    best_idx = np.argmax(mean_scores)\n",
        "    best_score = mean_scores[best_idx]\n",
        "    best_n_features = best_idx + 1\n",
        "    plt.axvline(best_n_features, color='red', linestyle='--', label=f'Best = {best_n_features} features')\n",
        "    plt.scatter(best_n_features, best_score, color='red')\n",
        "    plt.text(best_n_features + 10, best_score,\n",
        "             f'Best = {best_n_features}\\nROC AUC = {best_score:.3f}',\n",
        "             color='black', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    # Annotate point at 10 features \n",
        "    if n_features >= 10:\n",
        "        auc_at_10 = mean_scores[9]\n",
        "        plt.scatter(10, auc_at_10, color='green')\n",
        "        plt.text(10 + 10, auc_at_10,\n",
        "                 f'10 features\\nROC AUC = {auc_at_10:.3f}',\n",
        "                 color='darkgreen', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    plt.xlabel(\"Number of Features Selected\", fontsize=12)\n",
        "    plt.ylabel(\"Mean CV ROC_AUC\", fontsize=12)\n",
        "    plt.title(\"ROC_AUC vs Number of Features (with 95% CI)\", fontsize=14)\n",
        "    plt.xticks(np.arange(0, n_features + 1, step=100))\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_features_vs_cvscore_95ci(xgb_rfecv, cv_folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4a. Xgboost model training using top features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "import xgboost as xgb\n",
        "\n",
        "# Initialize result lists\n",
        "AUC = []\n",
        "AUC_sd = []\n",
        "AC = []\n",
        "AC_sd = []\n",
        "Pre = []\n",
        "Pre_sd = []\n",
        "Re = []\n",
        "Re_sd = []\n",
        "f1 = []\n",
        "f1_sd = []\n",
        "Best1 = []\n",
        "Best2 = []\n",
        "\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top 10 features\n",
        "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "\n",
        "# Use only the selected features for training\n",
        "XGB_train_selected = X_train[list(xgb_rfecv.get_feature_names_out())]\n",
        "XGB_test_selected = X_test[list(xgb_rfecv.get_feature_names_out())]\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [1, 3, 5, 7, 9]\n",
        "gs = GridSearchCV(estimator=xgboost_model,\n",
        "                  param_grid={'n_estimators': param_range1, 'max_depth': [1, 3, 5, 7, None], 'learning_rate': [0.01, 0.1, 0.2]},\n",
        "                  scoring='accuracy', cv=2, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(XGB_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = gs.best_params_\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "\n",
        "# Train the model with best hyperparameters\n",
        "XGB_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "XGB_model.fit(XGB_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = XGB_model.predict(XGB_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels= ['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.savefig(\"XGB_bestModel_ConfisionMatrix_FTIR_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"XGB_bestModel_ConfisionMatrix_FTIR_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=[str(label) for label in np.unique(y)]))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(XGB_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "all_tpr = []\n",
        "\n",
        "sd_list = []\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = XGB_model.fit(XGB_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(XGB_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "mean_sd = np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "# Plot mean ROC\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"XGB_bestModel_AUC_FTIR_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"XGB_bestModel_AUC_FTIR_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, XGB_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, XGB_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, XGB_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, XGB_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(gs.best_params_['n_estimators'])\n",
        "Best2.append('NaN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4b. XGB model for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# validation_data predictions \n",
        "validation_data_scaled = pd.DataFrame(scaler.transform(validation_data), columns=validation_data.columns, index=validation_data.index) # \n",
        "validation_data_selected = validation_data_scaled[list(xgb_rfecv.get_feature_names_out())]\n",
        "validation_pred = XGB_model.predict(validation_data_selected)\n",
        "print(\"Validation Data Predictions:\")\n",
        "for pid, pred in zip(validation_data_selected.index, validation_pred):\n",
        "    print(f\"{pid}: {'GBM' if pred == 1 else 'Healthy'}\") \n",
        "# # save validation predictions to csv\n",
        "validation_results = pd.DataFrame({'pid': validation_data_selected.index, 'Prediction': validation_pred})\n",
        "validation_results['Prediction'] = validation_results['Prediction'].apply(lambda x: 'GBM' if x == 1 else 'Healthy')\n",
        "validation_results.to_csv('XGB_FTIR_validation_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4a. Shaffiled class for best model -  Xgboost model training using top 10 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Ftir_data_train_shuffled1 = FTIR_data.copy()\n",
        "# Select 23 samples from 0 and change them to 1\n",
        "O_to_1_indices = Ftir_data_train_shuffled1[Ftir_data_train_shuffled1['target'] == 0].sample(n=24, random_state=1).index\n",
        "print(O_to_1_indices)\n",
        "# Select 23 samples from 1 and change them to 0\n",
        "I_to_O_indices = Ftir_data_train_shuffled1[Ftir_data_train_shuffled1['target'] == 1].sample(n=24, random_state=1).index\n",
        "print(I_to_O_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Ftir_data_train_shuffled1.loc[O_to_1_indices, 'target'] = 1\n",
        "Ftir_data_train_shuffled1.loc[I_to_O_indices, 'target'] = 0\n",
        "print(Ftir_data_train_shuffled1['target'].value_counts())\n",
        "print(Ftir_data_train_shuffled1[Ftir_data_train_shuffled1['target'] == 0].index)\n",
        "print(Ftir_data_train_shuffled1[Ftir_data_train_shuffled1['target'] == 1].index)\n",
        "# split data into train and test 80:20 ratio  \n",
        "X = Ftir_data_train_shuffled1.drop(['target',], axis = 1)\n",
        "y = Ftir_data_train_shuffled1['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "import xgboost as xgb\n",
        "\n",
        "# Initialize result lists\n",
        "AUC = []\n",
        "AUC_sd = []\n",
        "AC = []\n",
        "AC_sd = []\n",
        "Pre = []\n",
        "Pre_sd = []\n",
        "Re = []\n",
        "Re_sd = []\n",
        "f1 = []\n",
        "f1_sd = []\n",
        "Best1 = []\n",
        "Best2 = []\n",
        "\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top 10 features\n",
        "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "# Use only the selected features\n",
        "X_train_selected, X_test_selected = X_train[list(xgb_rfecv.get_feature_names_out())[:10]], X_test[list(xgb_rfecv.get_feature_names_out())[:10]]\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [1, 3, 5, 7, 9]\n",
        "gs = GridSearchCV(estimator=xgboost_model,\n",
        "                  param_grid={'n_estimators': param_range1, 'max_depth': [1, 3, 5, 7, None], 'learning_rate': [0.01, 0.1, 0.2]},\n",
        "                  scoring='accuracy', cv=2, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(X_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = gs.best_params_\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "\n",
        "# Train the model with best hyperparameters\n",
        "best_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "best_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = best_model.predict(X_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels= ['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.savefig(\"XGB_bestModel_shuffled_ConfusionMatrix_FTIR_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"XGB_bestModel_shuffled_ConfusionMatrix_FTIR_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=[str(label) for label in np.unique(y)]))\n",
        "\n",
        "# get AUC plot for the best model\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "probas = best_model.predict_proba(X_test_selected)\n",
        "fpr, tpr, _ = roc_curve(y_test, probas[:, 1], pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f'ROC (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"XGB_bestModel_shuffled_in_AUC_FTIR_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"XGB_bestModel_shuffled_in_AUC_FTIR_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
