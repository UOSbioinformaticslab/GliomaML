{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LDBBQxwMKz1"
      },
      "source": [
        "#miRNA ML for gliomas\n",
        "Jan, 2025\n",
        "Biniam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYwSTsKBD5k"
      },
      "source": [
        "\n",
        "\n",
        "> 1 - Loading the data\n",
        "\n",
        "> 2 - Modelling for Random Forest\n",
        "\n",
        "> 3 - Modelling Knn\n",
        "\n",
        "> 4 - Modelling for XGB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load processed Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read miRNA data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "miRNA_data_corrected = pd.read_csv('combined_miRNAs_scaled.csv')\n",
        "miRNA_data_corrected.rename(columns={'Unnamed: 0': 'pid'}, inplace=True)  # \n",
        "miRNA_data_corrected.set_index('pid', inplace=True)  # set pid as index\n",
        "# # # subset last 17 rows\n",
        "validation_miRNA_data = miRNA_data_corrected.iloc[-17:, :]  # last 17 rows\n",
        "trainging_miRNA_data = miRNA_data_corrected.iloc[:-17, :]  # all but last 17 rows\n",
        "trainging_miRNA_data['target'] = np.where(trainging_miRNA_data.index.str.contains('H'), 0, 1)\n",
        "\n",
        "# # # save to csv \n",
        "trainging_miRNA_data.to_csv('miRNA_trainging_data.csv', index=True)\n",
        "validation_miRNA_data.to_csv('miRNA_validation_data.csv', index=True)\n",
        "\n",
        "validation_miRNA_data.drop(columns=['dataset'], inplace=True)\n",
        "trainging_miRNA_data.drop(columns=['dataset'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count Healthy vs GBM in trainging_miRNA_data\n",
        "print(trainging_miRNA_data['target'].value_counts())\n",
        "X = trainging_miRNA_data.drop(['target',], axis = 1)\n",
        "y = trainging_miRNA_data['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForest Feature Selection Processs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "cv_split = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_rfecv = RFECV(\n",
        "    estimator=rf,\n",
        "    step=1,\n",
        "    min_features_to_select=1,\n",
        "    cv=cv_split,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "rf_rfecv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal number of features: {rf_rfecv.n_features_}\")\n",
        "print(f\"Optimal features: {list(rf_rfecv.get_feature_names_out())}\")\n",
        "\n",
        "rf_rfecv.cv_results_\n",
        "\n",
        "def plot_features_vs_cvscore_95ci(rfecv_model, cv_folds=5):\n",
        "    n_features = len(rfecv_model.cv_results_[\"mean_test_score\"])\n",
        "    mean_scores = rfecv_model.cv_results_[\"mean_test_score\"]\n",
        "    std_scores = rfecv_model.cv_results_[\"std_test_score\"]\n",
        "\n",
        "    # Calculate 95% CI\n",
        "    ci = 1.96 * (std_scores / np.sqrt(cv_folds))\n",
        "\n",
        "    x_range = range(1, n_features + 1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(x_range, mean_scores, marker='o', label='Mean ROC AUC')\n",
        "    plt.fill_between(x_range,\n",
        "                 np.clip(mean_scores - ci, 0, 1),\n",
        "                 np.clip(mean_scores + ci, 0, 1),\n",
        "                 color='lightblue',\n",
        "                 alpha=0.4,\n",
        "                 label='95% Confidence Interval')\n",
        "    # Annotate best feature point\n",
        "    best_idx = np.argmax(mean_scores)\n",
        "    best_score = mean_scores[best_idx]\n",
        "    best_n_features = best_idx + 1\n",
        "    plt.axvline(best_n_features, color='red', linestyle='--', label=f'Best = {best_n_features} features')\n",
        "    plt.scatter(best_n_features, best_score, color='red')\n",
        "    plt.text(best_n_features + 10, best_score,\n",
        "             f'Best = {best_n_features}\\nROC AUC = {best_score:.3f}',\n",
        "             color='black', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    # Annotate point at 10 features \n",
        "    if n_features >= 10:\n",
        "        auc_at_10 = mean_scores[9]\n",
        "        plt.scatter(10, auc_at_10, color='green')\n",
        "        plt.text(10 + 10, auc_at_10,\n",
        "                 f'10 features\\nROC AUC = {auc_at_10:.3f}',\n",
        "                 color='darkgreen', fontsize=8, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    plt.xlabel(\"Number of Features Selected\", fontsize=12)\n",
        "    plt.ylabel(\"Mean CV ROC_AUC\", fontsize=12)\n",
        "    plt.title(\"ROC_AUC vs Number of Features (with 95% CI)\", fontsize=14)\n",
        "\n",
        "    plt.xticks(np.arange(0, n_features + 1, step=50))\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_features_vs_cvscore_95ci(rf_rfecv, cv_folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2a. RandomForest with Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize result lists\n",
        "AUC, AUC_sd, AC, AC_sd, Pre, Pre_sd, Re, Re_sd, f1, f1_sd, Best1, Best2 = ([] for _ in range(12))\n",
        "\n",
        "# Reset indices of DataFrame\n",
        "X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# # Perform RFE with Random Forest to select top 10 features\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "selector = RFE(rf, n_features_to_select=10)\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "top_10_features_rf = X_train.columns[selector.support_]\n",
        "print(\"Top 10 Selected Features:\", top_10_features_rf)\n",
        "\n",
        "# # Use only the selected features\n",
        "rf_train_selected, rf_test_selected = X_train[top_10_features_rf], X_test[top_10_features_rf]\n",
        "\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [1, 2, 3, 5, 10, 20, None]}\n",
        "gs = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                  param_grid=param_grid,\n",
        "                  scoring='accuracy', cv=5, refit=True, n_jobs=-1)\n",
        "gs.fit(rf_train_selected, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = gs.best_params_\n",
        "print(f'Best Parameters: {best_params}')\n",
        "\n",
        "# Train final model\n",
        "model_rf = RandomForestClassifier(**best_params, random_state=42)\n",
        "model_rf.fit(rf_train_selected, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "test_pred = model_rf.predict(rf_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels=['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.savefig(\"RF_bestModel_ConfusionMatrix_miRNA_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"RF_bestModel_ConfusionMatrix_miRNA_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=['Healthy', 'GBM']))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(rf_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "sd_list = []\n",
        "\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = model_rf.fit(rf_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(rf_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_auc, mean_sd = auc(mean_fpr, mean_tpr), np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"RF_bestModel_AUC_miRNA_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"RF_bestModel_AUC_niRNA_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, rf_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, rf_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, rf_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, rf_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(best_params['n_estimators'])\n",
        "Best2.append(best_params['max_depth'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict Using External Validation miRNA batch corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict on validation set\n",
        "validation_miRNA_data_rf = validation_miRNA_data[top_10_features_rf]\n",
        "validation_pred = model_rf.predict(validation_miRNA_data_rf)\n",
        "# predictions to csv\n",
        "validation_miRNA_data_rf['predicted'] = validation_pred\n",
        "validation_miRNA_data_rf.to_csv('miRNA_validation_predictions.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3a. Knn model training using top-10 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recressive Feature Elimination\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize result lists\n",
        "AUC = []\n",
        "AUC_sd = []\n",
        "AC = []\n",
        "AC_sd = []\n",
        "Pre = []\n",
        "Pre_sd = []\n",
        "Re = []\n",
        "Re_sd = []\n",
        "f1 = []\n",
        "f1_sd = []\n",
        "Best1 = []\n",
        "Best2 = []\n",
        "\n",
        "# Reset indices of the DataFrame\n",
        "X = X.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top 10 features\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "selector = RFE(rf, n_features_to_select=10)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "top_10_features = X_train.columns[selector.support_]\n",
        "print(\"Top 10 Selected Features:\", top_10_features)\n",
        "\n",
        "#Use only the selected features for training\n",
        "knn_train_selected = X_train[top_10_features]\n",
        "knn_test_selected = X_test[top_10_features]\n",
        "\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [2, 3, 4, 5, 7, 9]\n",
        "gs = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                  param_grid=[{'n_neighbors': param_range1, 'metric': ['minkowski']}],\n",
        "                  scoring='accuracy', cv=5, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(knn_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_n_neighbors = gs.best_params_['n_neighbors']\n",
        "print(f'Best n_neighbors: {best_n_neighbors}')\n",
        "\n",
        "# Evaluate model using the best parameters\n",
        "model_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "model_knn.fit(knn_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = model_knn.predict(knn_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels=['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=['Healthy', 'GBM']))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(knn_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "all_tpr = []\n",
        "\n",
        "sd_list = []\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = model_knn.fit(knn_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(knn_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "mean_sd = np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "# Plot mean ROC\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, knn_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, knn_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, knn_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, knn_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(gs.best_params_['n_neighbors'])\n",
        "Best2.append('NaN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict on validation set\n",
        "validation_miRNA_data_knn = validation_miRNA_data[top_10_features]\n",
        "validation_pred_knn = model_knn.predict(validation_miRNA_data_knn)\n",
        "# predictions to csv\n",
        "validation_miRNA_data_knn['predicted'] = validation_pred_knn\n",
        "validation_miRNA_data_knn.to_csv('miRNA_validation_predictions_knn.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGB Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize model\n",
        "\n",
        "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "cv_split = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "\n",
        "xgb_rfecv = RFECV(\n",
        "    estimator=xgboost_model,\n",
        "    step=1,\n",
        "    min_features_to_select=1,\n",
        "    cv=cv_split,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_rfecv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal number of features: {xgb_rfecv.n_features_}\")\n",
        "print(f\"Optimal features: {list(xgb_rfecv.get_feature_names_out())}\")\n",
        "\n",
        "xgb_rfecv.cv_results_\n",
        "\n",
        "def plot_features_vs_cvscore_95ci(rfecv_model, cv_folds=5):\n",
        "    n_features = len(rfecv_model.cv_results_[\"mean_test_score\"])\n",
        "    mean_scores = rfecv_model.cv_results_[\"mean_test_score\"]\n",
        "    std_scores = rfecv_model.cv_results_[\"std_test_score\"]\n",
        "\n",
        "    # Calculate 95% CI\n",
        "    ci = 1.96 * (std_scores / np.sqrt(cv_folds))\n",
        "\n",
        "    x_range = range(1, n_features + 1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(x_range, mean_scores, marker='o', label='Mean ROC AUC')\n",
        "    plt.fill_between(x_range,\n",
        "                 np.clip(mean_scores - ci, 0, 1),\n",
        "                 np.clip(mean_scores + ci, 0, 1),\n",
        "                 color='lightblue',\n",
        "                 alpha=0.4,\n",
        "                 label='95% Confidence Interval')\n",
        "\n",
        "    # Annotate best feature point\n",
        "    best_idx = np.argmax(mean_scores)\n",
        "    best_score = mean_scores[best_idx]\n",
        "    best_n_features = best_idx + 1\n",
        "    plt.axvline(best_n_features, color='red', linestyle='--', label=f'Best = {best_n_features} features')\n",
        "    plt.scatter(best_n_features, best_score, color='red')\n",
        "    plt.text(best_n_features + 10, best_score,\n",
        "             f'Best = {best_n_features}\\nROC AUC = {best_score:.3f}',\n",
        "             color='black', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "\n",
        "\n",
        "    plt.xlabel(\"Number of Features Selected\", fontsize=12)\n",
        "    plt.ylabel(\"Mean CV ROC_AUC\", fontsize=12)\n",
        "    plt.title(\"ROC_AUC vs Number of Features (with 95% CI)\", fontsize=14)\n",
        "\n",
        "    plt.xticks(np.arange(0, n_features + 1, step=50))\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_features_vs_cvscore_95ci(xgb_rfecv, cv_folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4a. Xgboost model training using top 10 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Initialize result lists\n",
        "AUC = []\n",
        "AUC_sd = []\n",
        "AC = []\n",
        "AC_sd = []\n",
        "Pre = []\n",
        "Pre_sd = []\n",
        "Re = []\n",
        "Re_sd = []\n",
        "f1 = []\n",
        "f1_sd = []\n",
        "Best1 = []\n",
        "Best2 = []\n",
        "\n",
        "# Reset indices of the DataFrame\n",
        "X = X.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)\n",
        "\n",
        "X = X.astype('float64')\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top 10 features\n",
        "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Perform RFE with XGBoost as the estimator\n",
        "selector = RFE(xgboost_model, n_features_to_select=10)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "top_10_features_xgb = X_train.columns[selector.support_]\n",
        "print(\"Top 10 Selected Features:\", top_10_features_xgb)\n",
        "\n",
        "# Use only the selected features for training\n",
        "\n",
        "xgb_train_selected = X_train[top_10_features_xgb]\n",
        "xgb_test_selected = X_test[top_10_features_xgb]\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [50, 100, 200]\n",
        "gs = GridSearchCV(estimator=xgboost_model,\n",
        "                  param_grid={'n_estimators': param_range1, 'max_depth': [1, 3, 5, 7, None], 'learning_rate': [0.08, 0.15, 0.2]},\n",
        "                  scoring='accuracy', cv=5, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(xgb_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = gs.best_params_\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "\n",
        "# Train the model with best hyperparameters\n",
        "model_xgb = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model_xgb.fit(xgb_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = model_xgb.predict(xgb_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_mat, display_labels=['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=['Healthy', 'GBM']))\n",
        "\n",
        "# ROC/AUC Calculation for Cross-Validation\n",
        "cv = list(StratifiedKFold(n_splits=5).split(xgb_train_selected, y_train))\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "all_tpr = []\n",
        "\n",
        "sd_list = []\n",
        "for i, (train_idx, test_idx) in enumerate(cv):\n",
        "    probas = model_xgb.fit(xgb_train_selected.iloc[train_idx], y_train.iloc[train_idx]).predict_proba(xgb_train_selected.iloc[test_idx])\n",
        "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], probas[:, 1], pos_label=1)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "    sd_list.append(roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "mean_sd = np.std(sd_list)\n",
        "\n",
        "print(f'CV AUC: {mean_auc:.3f} +/- {mean_sd:.3f}')\n",
        "\n",
        "# Plot mean ROC\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--', label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "AUC.append(mean_auc)\n",
        "AUC_sd.append(mean_sd)\n",
        "\n",
        "# Nested Cross-Validation with Multiple Metrics\n",
        "scores1 = cross_val_score(gs, xgb_train_selected, y_train, scoring='accuracy', cv=5)\n",
        "scores2 = cross_val_score(gs, xgb_train_selected, y_train, scoring='precision', cv=5)\n",
        "scores3 = cross_val_score(gs, xgb_train_selected, y_train, scoring='recall', cv=5)\n",
        "scores4 = cross_val_score(gs, xgb_train_selected, y_train, scoring='f1', cv=5)\n",
        "\n",
        "print(f'cv accuracy: {np.mean(scores1):.3f} +/- {np.std(scores1):.3f}')\n",
        "print(f'cv precision: {np.mean(scores2):.3f} +/- {np.std(scores2):.3f}')\n",
        "print(f'cv recall: {np.mean(scores3):.3f} +/- {np.std(scores3):.3f}')\n",
        "print(f'cv f1: {np.mean(scores4):.3f} +/- {np.std(scores4):.3f}')\n",
        "\n",
        "AC.append(np.mean(scores1))\n",
        "AC_sd.append(np.std(scores1))\n",
        "Pre.append(np.mean(scores2))\n",
        "Pre_sd.append(np.std(scores2))\n",
        "Re.append(np.mean(scores3))\n",
        "Re_sd.append(np.std(scores3))\n",
        "f1.append(np.mean(scores4))\n",
        "f1_sd.append(np.std(scores4))\n",
        "Best1.append(gs.best_params_['n_estimators'])\n",
        "Best2.append('NaN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict on validation set\n",
        "validation_miRNA_data_xgb = validation_miRNA_data[top_10_features_xgb]\n",
        "validation_pred_xgb = model_xgb.predict(validation_miRNA_data_xgb)\n",
        "# predictions to csv\n",
        "validation_miRNA_data_xgb['predicted'] = validation_pred_xgb\n",
        "validation_miRNA_data_xgb.to_csv('miRNA_validation_predictions_xgb.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shuffling classes to test using best model RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count the number of 0s and 1s in the target column\n",
        "miRNA_data_shuffled = trainging_miRNA_data.copy()\n",
        "\n",
        "# Select samples from 0 and change them to 1\n",
        "O_to_1_indices = miRNA_data_shuffled[miRNA_data_shuffled['target'] == 0].sample(n=23, random_state=2).index\n",
        "print(O_to_1_indices)\n",
        "# Select samples from 1 and change them to 0\n",
        "I_to_O_indices = miRNA_data_shuffled[miRNA_data_shuffled['target'] == 1].sample(n=23, random_state=2).index\n",
        "print(I_to_O_indices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "miRNA_data_shuffled.loc[O_to_1_indices, 'target'] = 1\n",
        "miRNA_data_shuffled.loc[I_to_O_indices, 'target'] = 0\n",
        "print(miRNA_data_shuffled['target'].value_counts())\n",
        "print(miRNA_data_shuffled[miRNA_data_shuffled['target'] == 0].index)\n",
        "print(miRNA_data_shuffled[miRNA_data_shuffled['target'] == 1].index)\n",
        "# split data into train and test 80:20 ratio  \n",
        "X = miRNA_data_shuffled.drop(['target',], axis = 1)\n",
        "y = miRNA_data_shuffled['target']\n",
        "\n",
        "print(miRNA_data_shuffled['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize result lists\n",
        "AUC, AUC_sd, AC, AC_sd, Pre, Pre_sd, Re, Re_sd, f1, f1_sd, Best1, Best2 = ([] for _ in range(12))\n",
        "\n",
        "# Reset indices of DataFrame\n",
        "X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Use only the selected features for training\n",
        "X_train_selected = X_train[top_10_features]\n",
        "X_test_selected = X_test[top_10_features]\n",
        "\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "param_range1 = [2, 3, 4, 5, 7, 9]\n",
        "gs = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                  param_grid=[{'n_neighbors': param_range1, 'metric': ['minkowski']}],\n",
        "                  scoring='accuracy', cv=5, refit=True, n_jobs=-1)\n",
        "gs = gs.fit(knn_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_n_neighbors = gs.best_params_['n_neighbors']\n",
        "print(f'Best n_neighbors: {best_n_neighbors}')\n",
        "\n",
        "# Evaluate model using the best parameters\n",
        "model_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "model_knn.fit(X_train_selected, y_train)\n",
        "\n",
        "# Test the model on the test set\n",
        "test_pred = model_knn.predict(X_test_selected)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion_mat = confusion_matrix(y_test, test_pred)\n",
        "# use display_labels GBM = 1, Healthy = 0, for the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['Healthy', 'GBM'])\n",
        "disp.plot()\n",
        "plt.savefig(\"knn_bestModel_shuffled_ConfusionMatrix_miRNA_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"knn_bestModel_shuffled_ConfusionMatrix_miRNA_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, test_pred, target_names=['Healthy', 'GBM']))\n",
        "\n",
        "# get AUC plot for the best model\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "probas = model_knn.predict_proba(X_test_selected)\n",
        "fpr, tpr, _ = roc_curve(y_test, probas[:, 1], pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f'ROC (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='Random guessing')\n",
        "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='Perfect performance')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"knn_bestModel_shuffled_in_AUC_miRNA_HG.png\", format=\"png\", dpi=600)\n",
        "plt.savefig(\"knn_bestModel_shuffled_in_AUC_miRNA_HG.svg\", format=\"svg\", dpi=600)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
